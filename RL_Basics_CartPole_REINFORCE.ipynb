{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1JqtEuNGMOongRO/+bSCd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Policy-Gradient Algorithms on *CartPole* — Notebook Guide\n",
        "\n",
        "This single **Jupyter notebook** walks through two classic Monte-Carlo policy-gradient methods implemented in PyTorch:\n",
        "\n",
        "| Section | Algorithm | Baseline Type | Networks Trained |\n",
        "|---------|-----------|---------------|------------------|\n",
        "| **REINFORCE** | Vanilla Monte-Carlo Policy Gradient | Moving average of episode returns | Policy π |\n",
        "| **REINFORCE + Critic** | One-step Actor-Critic | Learned value function V(s) | Policy π and Value V |\n",
        "\n",
        "The environment is **Gymnasium’s `CartPole-v1`**—simple enough to run quickly, yet rich enough to illustrate how returns and advantages drive policy updates.\n",
        "\n",
        "---\n",
        "\n",
        "## Key RL Concepts Illustrated\n",
        "\n",
        "| Concept | Where to look | What it means |\n",
        "|---------|---------------|---------------|\n",
        "| **Episode roll-out** | `while not done:` loop | Sample actions \\(a_t \\sim \\pi_\\theta(s_t)\\) until the pole falls or the time limit expires. |\n",
        "| **Discounted return \\(G_t\\)** | `for rt in reversed(rewards):` | \\(G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots\\) with \\(\\gamma = 0.99\\). |\n",
        "| **Baseline & Advantage** | *Plain* REINFORCE: moving-average baseline.<br>*Actor-Critic*: learn \\(V_\\phi(s)\\). | Advantages reduce gradient variance:<br>\\(A_t = G_t - b_t\\) or \\(A_t = G_t - V_\\phi(s_t)\\). |\n",
        "| **Loss functions** | Policy loss for both:<br>\\(\\mathcal{L}_\\pi = -\\sum_t \\log\\pi_\\theta(a_t\\mid s_t) A_t\\)<br>Value loss (actor-critic only):<br>\\(\\mathcal{L}_V = \\lVert V_\\phi(s_t) - G_t\\rVert_2^2\\) | Drives π to favor high-advantage actions; trains V to approximate returns. |\n",
        "| **Optimization** | Separate Adam optimizers | One for policy, one for critic (when present), stepped once per episode. |\n",
        "\n",
        "---\n",
        "\n",
        "## Result\n",
        "Loss: -12    Return: 500    Episode length: 500"
      ],
      "metadata": {
        "id": "SjoPidOSlCew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_rA74DHrQtC",
        "outputId": "e5e777db-ab7e-475c-876c-96f230d2ad3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gymnasium\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "4Es2ljOJrV7T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'CartPole-v1'\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "# getting action space\n",
        "print(f\"action space: {env.action_space}\")\n",
        "s0, a0 = env.reset()\n",
        "print(f\"initial state: {s0}\")\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "CCRbD8AGryI3",
        "outputId": "ae05baad-af66-4935-c3c3-2aa82b6a779f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action space: Discrete(2)\n",
            "initial state: [0.04586143 0.04797461 0.01249856 0.02462727]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-67fed06e-de67-47d2-a23a-fee846d3a2fb\" class=\"ndarray_repr\"><pre>ndarray (400, 600, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-67fed06e-de67-47d2-a23a-fee846d3a2fb button').onclick = (e) => {\n",
              "        document.querySelector('#id-67fed06e-de67-47d2-a23a-fee846d3a2fb').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-67fed06e-de67-47d2-a23a-fee846d3a2fb button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Model with No Value Model\n",
        "\n",
        "REINFORCE algorithm, simplest and strong baseline\n",
        "* Policy Gradient\n",
        "* baseline is moving average of previous returns"
      ],
      "metadata": {
        "id": "20ptTVNCy3J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, state_dim, action_space_cardinality, dropout_rate=0.2):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(state_dim, 32*state_dim)\n",
        "    self.l2 = nn.Linear(32*state_dim, state_dim)\n",
        "    self.l3 = nn.Linear(state_dim, action_space_cardinality)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.l1(self.dropout(state))\n",
        "    x = F.relu(x)\n",
        "    x = self.l2(self.dropout(x))\n",
        "    # x = F.relu(x)\n",
        "    x = self.l3(self.dropout(x))\n",
        "    return self.softmax(x)\n",
        "\n",
        "\n",
        "pi = Policy(env.observation_space.shape[0], env.action_space.n, dropout_rate=0.0)\n",
        "probs = pi(torch.tensor(s0))\n",
        "at = torch.distributions.Categorical(probs).sample()"
      ],
      "metadata": {
        "id": "zqfAa_6oy2o2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(pi.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "sdmeDWrm9yvc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pi = pi.to(device)\n",
        "baseline = 0.0\n",
        "for ep in range(500):\n",
        "  # Play\n",
        "  done = False\n",
        "  st, _ = env.reset()\n",
        "  states = []\n",
        "  rewards = []\n",
        "  actions = []\n",
        "  while not done:\n",
        "    with torch.no_grad():          # ← graph not kept\n",
        "      inputs = torch.tensor(st).to(device)\n",
        "      probs = pi(inputs)\n",
        "      at = torch.distributions.Categorical(probs).sample()\n",
        "      next_s, rt, terminated, truncated, _ = env.step(at.detach().cpu().item())\n",
        "      done = terminated or truncated\n",
        "      rewards.append(rt)\n",
        "      actions.append(at)\n",
        "      states.append(st)\n",
        "      st = next_s\n",
        "\n",
        "  # Compute Return\n",
        "  gamma = 0.99\n",
        "  ret = 0\n",
        "  Gt = []\n",
        "  for _i, rt in enumerate(reversed(rewards)):\n",
        "    ret = rt + gamma*ret\n",
        "    Gt.insert(0, ret)\n",
        "\n",
        "  Gt = torch.tensor(Gt)\n",
        "  baseline = 0.1*Gt.mean() + 0.9*baseline\n",
        "  advantages = Gt - baseline\n",
        "\n",
        "  # calculate loss\n",
        "  optimizer.zero_grad()\n",
        "  log_probs = torch.stack([torch.distributions.Categorical(pi(torch.tensor(st).to(device))).log_prob(at) for st, at in zip(states, actions)])\n",
        "  loss = -(log_probs*advantages.to(device)).sum()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if ep % 50 == 0:\n",
        "    print(f\"Loss: {loss:.00f}\\tReturn: {Gt[0]:.0f}\\tNumber of actions: {len(rewards)}\")\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAmDoEMS1si6",
        "outputId": "78f81449-b8b9-4a77-9850-3d4ec0f96d73"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 61\tReturn: 13\tNumber of actions: 14\n",
            "----------------------\n",
            "Loss: 4\tReturn: 21\tNumber of actions: 24\n",
            "----------------------\n",
            "Loss: 110\tReturn: 37\tNumber of actions: 46\n",
            "----------------------\n",
            "Loss: -257\tReturn: 24\tNumber of actions: 27\n",
            "----------------------\n",
            "Loss: 4730\tReturn: 96\tNumber of actions: 326\n",
            "----------------------\n",
            "Loss: 2588\tReturn: 97\tNumber of actions: 351\n",
            "----------------------\n",
            "Loss: -1087\tReturn: 94\tNumber of actions: 279\n",
            "----------------------\n",
            "Loss: -1399\tReturn: 96\tNumber of actions: 312\n",
            "----------------------\n",
            "Loss: 120\tReturn: 99\tNumber of actions: 500\n",
            "----------------------\n",
            "Loss: 421\tReturn: 99\tNumber of actions: 500\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Model with Value Model\n",
        "Introducing Actor-Critic (A2C)\n",
        "\n",
        "Using state Value V(sₜ)\n",
        "\n",
        "Aₜ = Gₜ - V(sₜ)"
      ],
      "metadata": {
        "id": "lK8Xgdpcn1Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, state_dim, action_space_cardinality, dropout_rate=0.2):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(state_dim, 32*state_dim)\n",
        "    self.l2 = nn.Linear(32*state_dim, state_dim)\n",
        "    self.l3 = nn.Linear(state_dim, action_space_cardinality)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.l1(self.dropout(state))\n",
        "    x = F.relu(x)\n",
        "    x = self.l2(self.dropout(x))\n",
        "    # x = F.relu(x)\n",
        "    x = self.l3(self.dropout(x))\n",
        "    return self.softmax(x)\n",
        "\n",
        "\n",
        "pi = Policy(env.observation_space.shape[0], env.action_space.n, dropout_rate=0.0)\n",
        "probs = pi(torch.tensor(s0))\n",
        "at = torch.distributions.Categorical(probs).sample()"
      ],
      "metadata": {
        "id": "uRSaLpF4BXN9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueModel(nn.Module):\n",
        "  def __init__(self, state_dim):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(state_dim, 32*state_dim)\n",
        "    self.l2 = nn.Linear(32*state_dim, 1)\n",
        "\n",
        "\n",
        "  def forward(self, s):\n",
        "    x = self.l1(s)\n",
        "    x = F.relu(x)\n",
        "    x = self.l2(x)\n",
        "    return x\n",
        "\n",
        "V = ValueModel(env.observation_space.shape[0])"
      ],
      "metadata": {
        "id": "hnZq36tPn6ud"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_optimizer = torch.optim.AdamW(pi.parameters(), lr = 0.001)\n",
        "v_optimizer = torch.optim.AdamW(V.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "5gLgNIeKk5Sv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pi = pi.to(device)\n",
        "V = V.to(device)\n",
        "baseline = 0.0\n",
        "for ep in range(500):\n",
        "  \"\"\"\n",
        "  Play\n",
        "  \"\"\"\n",
        "  done = False\n",
        "  st, _ = env.reset()\n",
        "  states = []\n",
        "  rewards = []\n",
        "  actions = []\n",
        "  while not done:\n",
        "    with torch.no_grad():          # ← graph not kept\n",
        "      inputs = torch.tensor(st).to(device)\n",
        "      probs = pi(inputs)\n",
        "      at = torch.distributions.Categorical(probs).sample()\n",
        "    next_s, rt, terminated, truncated, _ = env.step(at.detach().cpu().item())\n",
        "    done = terminated or truncated\n",
        "    rewards.append(rt)\n",
        "    actions.append(at)\n",
        "    states.append(st)\n",
        "    st = next_s\n",
        "\n",
        "  \"\"\"\n",
        "  Compute Return\n",
        "  \"\"\"\n",
        "  # critic values\n",
        "  states_tensor = torch.stack([torch.tensor(st, dtype=torch.float32, device=device) for st in states])\n",
        "  values = V(states_tensor)\n",
        "\n",
        "  # Gt\n",
        "  gamma = 0.99\n",
        "  ret = 0\n",
        "  Gt = []\n",
        "  for _i, rt in enumerate(reversed(rewards)):\n",
        "    ret = rt + gamma*ret\n",
        "    Gt.insert(0, ret)\n",
        "\n",
        "  # Advantages\n",
        "  Gt = torch.tensor(Gt, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  calculate loss\n",
        "  \"\"\"\n",
        "  # Value model\n",
        "  v_optimizer.zero_grad()\n",
        "  mse_loss = F.mse_loss(values.squeeze(), Gt, reduction='mean')\n",
        "  mse_loss.backward()\n",
        "  v_optimizer.step()\n",
        "\n",
        "  # Policy\n",
        "  advantages = (Gt - values).detach() # no grad from policy into value model\n",
        "  pi_optimizer.zero_grad()\n",
        "  log_probs = torch.stack([torch.distributions.Categorical(pi(torch.tensor(st).to(device))).log_prob(at) for st, at in zip(states, actions)])\n",
        "  loss = -(log_probs*advantages.to(device)).sum()\n",
        "  loss.backward()\n",
        "  pi_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if ep % 50 == 0:\n",
        "    print(f\"Loss: {loss:.00f}\\tReturn: {Gt[0]:.0f}\\tNumber of actions: {len(rewards)}\")\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VghIf4jQojnx",
        "outputId": "b30be25d-3c4e-44dc-8716-c78e594941ce"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4707\tReturn: 21\tNumber of actions: 24\n",
            "----------------------\n",
            "Loss: 706\tReturn: 12\tNumber of actions: 13\n",
            "----------------------\n",
            "Loss: 4319\tReturn: 23\tNumber of actions: 26\n",
            "----------------------\n",
            "Loss: 6182\tReturn: 28\tNumber of actions: 32\n",
            "----------------------\n",
            "Loss: 192066\tReturn: 66\tNumber of actions: 108\n",
            "----------------------\n",
            "Loss: 1551140\tReturn: 92\tNumber of actions: 253\n",
            "----------------------\n",
            "Loss: 2943241\tReturn: 97\tNumber of actions: 341\n",
            "----------------------\n",
            "Loss: 6657498\tReturn: 99\tNumber of actions: 500\n",
            "----------------------\n",
            "Loss: 5623267\tReturn: 99\tNumber of actions: 500\n",
            "----------------------\n",
            "Loss: 43404\tReturn: 72\tNumber of actions: 127\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7P9ddwvD-bW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}