{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7sjsM36yeKJE/jPRoaJJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamhamed/Reinforcement_Learning/blob/master/RL_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to train a value-based agent—specifically a\n",
        "**Deep Q-Network (DQN)**—to land the lunar module safely in the classic\n",
        "control benchmark *LunarLander-v3*.  Although reinforcement learning for large\n",
        "language models typically relies on policy-gradient methods such as PPO (or\n",
        "newer variants like GRPO), DQN remains one of the foundational and most\n",
        "widely-cited algorithms in RL literature, making it a useful baseline for smaller, discrete-action tasks.\n",
        "\n",
        "\n",
        "| Value Function                  | Formal Definition                                         | Intuition — “What it tells you”                                           | How to Extract/Use a Policy                                                                                                                                        |                 |\n",
        "| ------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------- |\n",
        "| **State Value `V(s)`**          | **E\\_π \\[ ∑<sub>t = 0</sub>^∞ γᵗ rₜ \\| s₀ = s ]**         | “How good is it *to be in* this state under policy π?”                    | You still need a separate policy π(a \\| s) (e.g., an actor network). `V(s)` is often used only as a **baseline** to reduce variance or to rank states in planning. |                 |\n",
        "| **State–Action Value `Q(s,a)`** | **E\\_π \\[ ∑<sub>t = 0</sub>^∞ γᵗ rₜ \\| s₀ = s, a₀ = a ]** | “How good is it to take **this action** in this state and then follow π?” | A greedy (or soft-max) selection over `Q(s,a)` **is** the policy: <br/> • Discrete: `a* = argmax_a Q(s,a)` <br/> • β-softmax: \\`π(a                                | s) ∝ exp(Q/β)\\` |\n"
      ],
      "metadata": {
        "id": "aGv4A9cxVF-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_8B0ngtU6rN"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hhYB1FO9VB8M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'LunarLander-v3'\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "# getting action space\n",
        "print(f\"action space: {env.action_space}\")\n",
        "s0, a0 = env.reset()\n",
        "print(f\"initial state: {s0}\")\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "imf1FRgZVqpr",
        "outputId": "56b70d50-396a-4a4a-a16a-3a36ebb80346"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action space: Discrete(4)\n",
            "initial state: [ 0.0031745   1.4183462   0.32153386  0.33004856 -0.00367174 -0.07283233\n",
            "  0.          0.        ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-5ced81e4-9448-43c0-91bd-3ec538dcf2a0\" class=\"ndarray_repr\"><pre>ndarray (400, 600, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-5ced81e4-9448-43c0-91bd-3ec538dcf2a0 button').onclick = (e) => {\n",
              "        document.querySelector('#id-5ced81e4-9448-43c0-91bd-3ec538dcf2a0').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-5ced81e4-9448-43c0-91bd-3ec538dcf2a0 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "In actor-critic where the actor output is the probability of actions and hence we have `softmax` at the end of the network.\n",
        "\n",
        "In DQN (or general Q-learning), the network output is the value of taking action `aₜ` at state `sₜ`. Therefore we return the logits w/o the softmax"
      ],
      "metadata": {
        "id": "wu5_X3l6mPjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, state_space, action_space):\n",
        "    super().__init__()\n",
        "    self.ln1 = nn.Linear(state_space, 256)\n",
        "    self.ln2 = nn.Linear(256, 256)\n",
        "    self.ln3 = nn.Linear(256, action_space)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.ln1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.ln2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.ln3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "42-jPoLwVt_P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed‑size cyclic buffer for experience replay.\"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)  # (s, a, r, s_next, done)\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "IzNHzvFbXM51"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPLAY_CAPACITY     = 100_000\n",
        "MAX_FRAMES          = 200_000\n",
        "BATCH_SIZE          = 128\n",
        "gamma               = 0.995\n",
        "EPSILON_END         = 0.05\n",
        "EPSILON_DECAY       = 25_000\n",
        "TARGET_UPDATE_FREQ  = 1_000"
      ],
      "metadata": {
        "id": "_ne1Pwk2YOPE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qnet = QNetwork(env.observation_space.shape[0], env.action_space.n)\n",
        "qtarget = QNetwork(env.observation_space.shape[0], env.action_space.n)\n",
        "qtarget.load_state_dict(qnet.state_dict())\n",
        "optimizer = torch.optim.AdamW(qnet.parameters(), lr=5e-4)\n",
        "\n",
        "replay_buffer = ReplayBuffer(REPLAY_CAPACITY)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "qnet = qnet.to(device)\n",
        "qtarget = qtarget.to(device)\n",
        "qtarget = qtarget.eval()"
      ],
      "metadata": {
        "id": "VlMrXa4vXcdV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 1-step TD\n",
        "```\n",
        "qtarget = r + γ maxₐ′ Qθ(s′,a′)\n",
        "```\n",
        "\n",
        "and optimizing for\n",
        "```\n",
        "||r + γQ′ − Q||\n",
        "```"
      ],
      "metadata": {
        "id": "KndJre-hqjTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st, _ = env.reset()\n",
        "\n",
        "episode_reward = 0\n",
        "epsilon = 1\n",
        "for frame_idx in range(MAX_FRAMES):\n",
        "  epsilon = EPSILON_END + (1.0 - EPSILON_END) * np.exp(-frame_idx / EPSILON_DECAY)\n",
        "  if random.random() < epsilon:\n",
        "    at = random.randint(0, env.action_space.n-1)\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      _st = torch.tensor(st, device=device, dtype=torch.float32)\n",
        "      at = qnet(_st.unsqueeze(dim=0)).argmax(dim=-1).squeeze().item()\n",
        "\n",
        "  next_state, rt, terminated, truncated, _ = env.step(at)\n",
        "  done = terminated or truncated\n",
        "\n",
        "  # (s, a, r, s_next, done)\n",
        "  replay_buffer.push((st, at, rt, next_state, done))\n",
        "  episode_reward += rt\n",
        "  if done:\n",
        "    st, _ = env.reset()\n",
        "    print(f\"Frame {frame_idx:>7} | Episode reward: {episode_reward:>7.2f} | ε: {epsilon:>.3f}\")\n",
        "    episode_reward = 0.0\n",
        "  else:\n",
        "    st = next_state\n",
        "\n",
        "  if len(replay_buffer.buffer) >= BATCH_SIZE:\n",
        "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
        "    _states = torch.tensor(states, device=device, dtype=torch.float32)\n",
        "    _actions = torch.tensor(actions, device=device, dtype=torch.long).unsqueeze(1)\n",
        "    _next_states = torch.tensor(next_states, device=device, dtype=torch.float32)\n",
        "    _dones = torch.tensor(dones, device=device, dtype=torch.float32).unsqueeze(1)\n",
        "    _rewards = torch.tensor(rewards, device=device, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "\n",
        "    # get the qvals for actions chosen before, we want to optimize qnet to give best actions\n",
        "    qvals = qnet(_states).gather(1, _actions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # what is qvals for next states that is selected by qnet\n",
        "      best_action = qnet(_next_states).argmax(1, keepdim=True)\n",
        "      qvals_next = qtarget(_next_states).gather(-1, best_action)\n",
        "      qtargetvals = _rewards + gamma * qvals_next * (1-_dones)\n",
        "\n",
        "    loss = F.mse_loss(qvals, qtargetvals)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Target network sync\n",
        "    if frame_idx % TARGET_UPDATE_FREQ == 0:\n",
        "        qtarget.load_state_dict(qnet.state_dict())\n"
      ],
      "metadata": {
        "id": "wFGa4VzDXwGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "omkSeLs8jP2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}